---
title: "'Maarten van Rossem - De Podcast' - #330 Textanalysis of the podcast"
author: "Alex van Vorstenbosch"
date: today
abstract: "De 'Maarten van Rossem'-podcast is one of the most popular podcasts in The Netherlands, which has produced more than 300 episodes in the past 2 years. In this article, we produce some fun, and hopefully interesting, statistics about the podcast. This is done by collecting all the currently available podcasts from the web, and transcribing them using the newly available `Whisper` automated speech recognition model by [OpenAI](https://github.com/openai/whisper). After some initial statistical analysis of the podcast, finish our analysis by extracting topics from podcast using `BERTopic`, a Topic Modeling pipeline by [Maarten Grootendorst](https://github.com/MaartenGr/BERTopic). The goal of this project is first of all to have fun analysing the words of Maarten and Tom. I hope you'll learn something interesting from all this, and that you have fun along the way. (And I hope that Maarten and Tom would mention this project in the podcast!)"
abstract-title: "Analysing the Podcast"
number-sections: true
highlight-style: arrow
format:
  html:
    colorlinks: true
    embed-resources: true
    number-sections: true
    theme:
      light: default
      dark: darkly
    smooth-scroll: true
    toc: true
    toc-title: Table of Contents
code-overflow: wrap
code-fold: true
code-summary: "Show the code"
code-tools: 
    source: https://github.com/avvorstenbosch/Maarten_De_Podcast_Analysis
execute:
    warning: false
    error: false
title-block-banner: "#27445C"
jupyter: maarten
---

# Introduction and Code Setup {#sec-setup}
Hi! Welcome at the start of our journey through the data of the 'Maarten van Rossem' podcast.
As you will notice below, I've included all the code for this analysis in this document, so you can see exactly how the analysis was performed. Some may find this wonderful, while some may find this more annoying than anything else. To be fair, it does hinder the readability of the article a little. That is why the default setting is to hide the code. However, if you'd like to see all the code, you can click the `</> Code` tab to the right of the Title, and click `Show all code` to set all code chunks to be unfolded and visible.

It's also good to note, that if you are interested in the source-code of other elements of this project such as the webscraper, please click `View Source` in the same tab to visit my github profile. 

A final note is that at the top right of the page you will find a small button which you can use to toggle `dark-mode` for this article.

Alright, I said final note but this time I mean it:
This article was written for desktop viewing. It won't throw errors if you try to read this on a mobile device, but the formatting will be wonky and the interactive figures will be almost impossible to navigate. So if you're able to, please read this on a desktop device.

Now that we've handled these formalities, let's get started!

```{python}
# Base imports
import os
import re
import pickle
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
from matplotlib.colors import rgb2hex
import plotly.graph_objects as go

# Set the random seed for reproducability
import random
random_seed = 2112
random.seed(random_seed)
np.random.seed(random_seed)

# Set plotting settings
plt.style.use('seaborn-darkgrid')
plt.rcParams['figure.dpi'] = 125
px = 1/plt.rcParams['figure.dpi'] 
figsize= (int(1200*0.9), int(700*0.9)) #px
figsize_inch = (figsize[0]*px, figsize[1]*px)

# For Fitting distributions
from scipy.stats import norm, lognorm, gamma, linregress
from scipy.optimize import curve_fit
from statsmodels.nonparametric.kde import KDEUnivariate
from sklearn.metrics import mean_squared_error

# For simple text processing
from nltk.corpus import stopwords
from nltk.tokenize import RegexpTokenizer, sent_tokenize

# For complex text processing
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.decomposition import NMF, MiniBatchNMF, LatentDirichletAllocation 
from sklearn.preprocessing import normalize
from bertopic import BERTopic
from umap import UMAP
from hdbscan import HDBSCAN
from sentence_transformers import SentenceTransformer

# For making Wordclouds
from wordcloud import WordCloud

# For neat progress bars
from tqdm import tqdm

# For neat typehinting in Python
from typing import List, Union

# Here we define some usefull utility-functions
def write_pickle(path, object):
    """
    Pickle and save an object.

    Parameters
    ----------
    path : str
        Path to pickled file
    object : any
        An object to pickle
    """    
    with open(path, 'wb') as f:
        pickle.dump(object, f)

def read_pickle(path):
    """
    Read and return a pickled object.

    Parameters
    ----------
    path : str
        Path to pickled file

    Returns
    -------
    any unpickled object(s)
    """
    with open(path, 'rb') as f:
        object = pickle.load(f)
    return object

def read_txt_file(path):
    """
    read text file from path

    Parameters
    ----------
    path : str
        filepath

    Returns
    -------
    text : str
        contents of the file
    """    
    if not os.path.exists(path):
        return np.nan

    with open(path, "r") as f:
        text = f.read()
    return text
```

## Load the data
Here we load the data, which has been stored in a dataframe.
Below the cell, we've printed a short example of what the data looks like:

```{python}
data = pd.read_pickle("../extract_data/data.pickle")

# Sort by episode number
data = data.sort_values(["episode"]).reset_index(drop=True)

# Read Corpus
data["text"] = data["txt_path"].transform(lambda path: read_txt_file(path))

# Drop episodes that have not been transcribed yet
data = data.dropna(subset=["text"]).reset_index(drop=True)

# Check if file is a numbered episode
# We also skip the trailer as it is episode 0
data["is_episode"] = data["episode"].transform(lambda num: num > 0)

print_cols = ["titles", "date", "duration", "episode"]
pd.set_option('display.max_colwidth', 40)
data.tail(3)
```

# 'Goed Nieuws' - There is always a new episode around the corner {#sec-statistics}
Let's get started with our analysis! 
There are so many points were we could possibly start, so let's start with analysing the dataframe I prepared for us.

From the dataframe we can see that the number of 'episodes' on the channel is:
```{python}
#| output: asis
#| echo: false
print(f"{data.shape[0]} episodes.")
```

Great! But hold on, that's not quite right... I thought we were on something like episode 320?
The reason for all the extra episodes is that the podcast-channel also includes specials, trailers, and the likes:

```{python}
data_full = data.copy()

# create a simple mask for getting only the numbered episodes
is_ep = (data.is_episode==True)
data[~is_ep][print_cols].to_html(index=False)
```

For our analysis, it is better to ignore these specials, as they are not representative for typical podcast episodes. When we remove these episodes we're left with:

```{python}
#| output: asis
#| echo: false

# The standard dataframe will only contain the numbered episodes from now on
data = data_full[is_ep].reset_index(drop=True)
num_episodes = data.shape[0]
print(f"We have {num_episodes} episodes.")
print(f"However, the latest episode is {data.episode.max()}")
```
We are missing 1 episode, episode _'#116 - Joe Biden: 1 jaar'_ as this is the only episode that was released exclusively for paid subscribers to the podcast.

The collapsible cell below is something you'll encounter more often in this article. It's a convenient way for us to put interesting results into the file, while making sure we don't create massive clutter in the process.

::: {.callout-note collapse="true"}
## Click here to see the raw statistics for all the episodes on the channel!
```{python}
#| echo: false
data[print_cols].to_html(index=False)
```
:::

So as you may have seen above, the first real episode was:

```{python}
data.sort_values("date").iloc[0:1][print_cols].to_html()
```

That's quite the title! Maarten and Tom are not afraid to speak their mind, that's for sure!
So the first episode was released on the 7th of June 2020. This means, that the podcast has been running for a total of:

```{python}
#| output: asis
#| echo: false 
days_running = (max(data.date) - min(data.date)).days
print(f"{days_running} days.")
```

```{python}
#| output: asis
#| echo: false
mean_time_between_episodes = days_running/num_episodes
print(f"For the loyal listeners, that's good new! As we have {data.episode.max()} episodes at the time of writing. It means that the average waiting time for a new episode is only {mean_time_between_episodes:.2f} days! That is more than {7/mean_time_between_episodes:.0f} episodes per week over the span of 2 years, wow!")
```

It does probably help that Maarten and Tom can talk about pretty much anything, given that they are both a history and a current affairs podcast. However, releasing many episodes doesn't mean a lot if there is only little content in each episode. Now, I personally am very satisfied with amount of content released by Maarten and Tom, but perhaps we can quantify it? I wonder how much talking they actually do per episode? Let's find out in the next section!

# 'Beste mensen' - The rate at which content is being released has tripled!
Let's slowly start wading a little deeper into our sea of data:  'the contents of the episodes'. I've used the [`Whisper`](https://github.com/openai/whisper) deep neural network model to transcribe all the episodes of the podcast into written text documents. It's really quite perplexing how well this works. It took about 3 days of GPU time to transcribe all episodes of the podcast. Luckily for me, it was a rather cold weekend, so I could use these calculations the heat my appartement a little.
If you have some experience with Python and/or the commandline, it's possible to run the software on pretty much any computer, so I would advice you to see for yourself how good the results are. 

::: {.callout-caution}
## There is always room for improvement
While the results are really impressive, they are of course not perfect. The Word-Error-Rate (WER) for Dutch was estimated at 6.7%. Depending on the context, this WER might be higher or lower. For example, `Whisper` struggles with processing the episode jingle "Met van Rossem. Kunt u mij horen?", as demonstrated by this short selection of transcription examples:

* Met verlossem. Kunt u bijhouden?
* Met Verlossum. Kunt u mij horen?
* Het verhorsum. Kunt u mij horen?
* Het is van Lawson, kunt u mij horen?

This is something you will see more often in this analysis. My interpretation of why these errors happen is this: `Whisper` tries to use both the spectrogram of the audio as well as the textual context to generate the transcription, but if there is little relevant context to go by, the model needs to make an essentially 'random' guess to what could be meant. This is of course the case at for the start-of-episode jingle as it has no relevant context. Also, names are difficult without context. This is almost always the case for the names of people, except for example famous political figures which would have been included in the training data. Finally, the music and singing might further confuse the model.
:::

## How much content is there in each episode?
We could have a long discussion on what we consider to be content and how we want to measure it. Content as an abstract concept is hard to measure, so we just take the number of words spoken as a proxy for the content of each episode. This is easily understandable to anybody, has a clear and direct link to 'content', and best of all: this is easily visualised with a histogram!
```{python}
#| label: fig-wpe_dist
#| fig-cap: The distribution of Words-per-Episode for all the content released as part of the podcast
tokenizer = RegexpTokenizer(r"\w+")


def get_length(text):
    """
    Calculates length of a text by counting the number of tokens.

    Parameters
    ----------
    text : str
        Text to process.

    Returns
    -------
    _ : int
        Length of text.
    """
    tokens = tokenizer.tokenize(text)
    return len(tokens)


# Words-per-Episode
data["WPE"] = data["text"].transform(lambda text: get_length(text))
data_full["WPE"] = data_full["text"].transform(lambda text: get_length(text))


def plot_lengths_histogram(data, title, bins=12):
    """
    Plot and show a figure of the distribution of episode lenghts in a histogram.

    Parameters
    ----------
    data : pd.DataFrame
        Information about the episodes.
    title : str
        Second part of the figure title.
    bins : int, optional
        Number of bins in the histogram, by default 12
    """
    plt.figure(figsize=figsize_inch)
    plt.hist(data["WPE"], bins, edgecolor="#EAEAF2", linewidth=1, align="mid")
    plt.title(f"Length of Podcast in Words-Per-Episode - {title}")
    plt.xlabel("# of Words")
    plt.ylabel("Counts")
    plt.tight_layout()
    plt.show()


plot_lengths_histogram(
    data_full, title="All released episodes (specials included)", bins=20
)
```

As we can see in @fig-wpe_dist the content is quite varied. Some episodes have almost 16.000 words, while others have only about 2000. But on the whole, most episodes have around 6.000 words. As it turns out, the episodes with the most words are typically the specials, which is not suprising as these are also the longest episodes.

Plotting a histogram of course means that we aggregate our data into bins. This is easy to interpret but also means we are throwing away information. Another fun way to visualize the content, is to plot all the individual datapoints over time. Will we be able to spot any interesting trends?

```{python}
#| label: fig-wpe_time
#| fig-cap: The distribution of Words-per-Episode over time, shown together with the duration of episodes over time. A clear trend is visible where episodes at the start of the podcast lasted more than ~50 minutes, while in the second half of the podcast episode duration dropped to around ~30 minutes. At the same time we can see that the rate at which episodes were released increased rapidly, the density of datapoints increases by a lot!

data["duration-s"] = pd.to_timedelta(data["duration"]).dt.total_seconds()
data["duration-m"] = data["duration-s"] / 60.0


def plot_wpe_over_time(data):
    """
    Plot and show a figure of the Words per Episode over Time on one axis and the Duration over Time on the other axis.

    Parameters
    ----------
    data : pd.DataFrame
        Information about the episodes.
    """
    fig, ax = plt.subplots(figsize=figsize_inch)
    plt.title("Words-per-Episode and Episode Duration")
    ax.plot(data["date"], data["WPE"], alpha=0.5, label="Words-per-Episode")
    ax.set_xlabel("Date")
    ax.set_ylabel("Words_per_Episode (#)")
    ax.scatter(data["date"], data["WPE"], s=5, label="Individual datapoints")
    plt.xticks(rotation=70)
    ax.xaxis.set_major_locator(mdates.MonthLocator())
    ax.xaxis.set_major_formatter(mdates.DateFormatter("%Y-%b"))

    # second y-axis
    ax2 = ax.twinx()
    ax2.plot(data["date"], data["duration-m"], c="r", alpha=0.5, label="Duration")
    ax2.set_ylabel("Episode Duration (minutes)")

    # collect labels for legend
    lines, labels = ax.get_legend_handles_labels()
    lines2, labels2 = ax2.get_legend_handles_labels()
    ax2.legend(lines + lines2, labels + labels2, loc=0)


plot_wpe_over_time(data.sort_values(by=["date", "episode"]))
```

We see an interesting pattern. @fig-wpe_time shows us that the podcast started with a few long episodes per month, and slowly evolved into many shorter episodes per week. I belive that when Maarten and Tom started recording the podcast, they only did so on saturdays. Then they started spliting recordings into multiple episodes, and these days, they record multiple times a week. 

So the episodes became shorter, but also more frequent. The question is which of the two had a bigger effect? Are we getting less or more content each week?

Before we answer that question, I would like to revisit our histogram, as @fig-wpe_time has given us some usefull information and how to filter our data to make a histogram

### 'Doe maar (log)normaal...' -  Fitting statistical distributions to the histogram

We could see from the previous figure, that the structure of episodes has changed quite a bit over time. The early episodes were typically around an hour long, and in the summer of around 2021 this was brought back to around ~40 minutes. It is interesting to note that we can actually fit statistical distributions to the variation that we see, as we will do in the following figure. We select all episodes released after August 1st 2022, as the release schedule and episode length become more consistent after this point.

```{python}
#| label: fig-lognormal
#| fig-cap: "Fitting a Gamma and Lognormal distribution to the Words-per-Episode of the podcast shows that some interesting patterns emerge in the data. Both distributions seem plausible for the podcast."
def plot_lengths_histogram_fit(data, title, bins=12):
    """
    Plot a histogram of episode lengths fit statistical distributions.

    Parameters
    ----------
    data : pd.DataFrame
        Information about the episodes.
    title : str
        Second part of the figure title.
    bins : int, optional
        Number of bins in the histogram, by default 12

    Returns
    -------
    model_params : list
        List with the fit parameters for the statistical models.
    """
    model_params = []
    plt.figure(figsize=figsize_inch)
    plt.hist(data["WPE"], bins, density=True, edgecolor="#EAEAF2", linewidth=1)

    # fit the lognormal
    params = lognorm.fit(data["WPE"], floc=0)
    model_params.append(params)
    x = np.linspace(0, data["WPE"].max(), 100)
    lognormal_fitted = lognorm.pdf(x, *params)
    plt.plot(
        x,
        lognormal_fitted,
        color="g",
        label="Lognormal Distribution:\n"
        + r"  -$s$="
        + f"{round(params[0],2)}\n  -loc={round(params[1],2)}\n  -scale={round(params[2],2)}",
    )

    # fit the gamma
    params = gamma.fit(data["WPE"], floc=0)
    model_params.append(params)
    x = np.linspace(0, data["WPE"].max(), 100)
    gamma_fitted = gamma.pdf(x, *params)
    plt.plot(
        x,
        gamma_fitted,
        color="r",
        label="Gamma Distribution:\n"
        + r"  -$\alpha$="
        + f"{round(params[0],2)}\n  -loc={round(params[1],2)}\n  -scale={round(params[2],2)}",
    )

    plt.title(f"Length of Podcast in Words-Per-Episode  - {title}")
    plt.xlabel("# of Words")
    plt.ylabel("Probability Density")
    plt.legend()
    plt.tight_layout()
    plt.show()
    return model_params


data_second_half = data[data["date"] > "2021-08-1"]
model_params = plot_lengths_histogram_fit(
    data_second_half, title="Fitted Lognormal and Gamma Distribution", bins=20
)
```

::: {.callout-note collapse="true"}
## Extra info on the selection for this figure
If we take the full distribution of episodes, the resulting distribution becomes bimodal. This essentially means that the distribution originates from the combination of 2 seperate distributions. This is also visible in the lineplot, where the first episodes are clearly different from the last episodes.
:::

It is fun to see that the distribution of Words-Per-Episode fits these statistical distributions so well. The Gamma distribution succesfully models other problems such as:

* waiting times at a busstop
* the severity of insurance claims
* the load on webservers

The Lognormal distribution succesfully models problems such as:

* A country's income distribution
* The length of chess games
* Particle size distributions in various contexts

So perhaps we can add to these lists:

* the words-per-episode for the 'Maarten van Rossem' podcast 

::: {.callout-note collapse="true"}
## The statistics of the statistical distributions above.
If we look at the raw data, the mean and the standard deviation are given by:

```{python}
print(f"Mean = {round(data_second_half['WPE'].mean(), 2)}")
print(f"Standard deviation = {round(data_second_half['WPE'].std(), 2)}")
```

The best fit on this distribution for the Lognormal distributions gives:

```{python}
#| echo: false
params = model_params[0]
mu = np.log(params[2])
sigma = params[0]
print(f"Mean = {round(np.exp(mu+sigma**2/2), 2)}")
print(f"Standard deviation = {round(np.sqrt((np.exp(sigma**2)-1)*np.exp(2*mu+sigma**2)), 2)}")
```

The best fit on this distribution for the Gamma distributions gives:

```{python}
#| echo: false
params = model_params[1]
print(f"Mean = {round(params[0]*params[2], 2)}")
print(f"Standard deviation = {round(np.sqrt(params[0]*params[2]**2), 2)}")
```
:::

With that out of the way, lets answer the question at hand: Are we getting less or more content each week?

### The evolution of content over time
In @fig-wpe_time we looked at the content in each unique episode. But it might be more interesting to look at the content released over a set period of time, for example 'per week'. By doing this we can account for both the episodes becoming shorter and the episodes becoming more frequent. 
```{python}
#| label: fig-content_linear
#| fig-cap: "There is a clear rising trend in the weekly data, as illustrated by the linear fit. The trend indicates that the amount of weekly content has been increasing with about 144 words per week."


def plot_content_linear_fit(data):
    """
    Plot and show a figure of a linear fit to the amount of Content over Time grouped by Week.

    Parameters
    ----------
    data : pd.DataFrame
        Information about the episodes.

    Returns
    -------
    rmse_lineair_wpw: float
        The root mean square error of the linear fit.
    """
    START = data.iloc[0].date

    def weeks_since_start(date):
        """
        Calculate the number of weeks since START.

        START is defined outside the functions as the date of the first episode in the dataframe, typically episode 1.

        Parameters
        ----------
        date: pandas.timedelta
            The date for which to calculate the timedifference.

        Returns
        -------
        weeks_diff: int
            The number of weeks from START until date

        """
        time_diff = date - START
        weeks_diff = int(time_diff / np.timedelta64(1, "W"))
        return weeks_diff

    # Convert date to weeks since start
    data_plot = data.copy()
    data_plot = data_plot.sort_values("date").reset_index(drop=True)
    data_plot["weeks"] = data_plot.date.transform(lambda dt: weeks_since_start(dt))
    weeks = data_plot.weeks.unique()

    # Calculate weekly increase
    WPW = data_plot.groupby("weeks").WPE.sum()

    # Perform linear regression with intercept = 0
    weeks = np.array(weeks)
    a, b, r, p, se = linregress(weeks, WPW)
    rmse_lineair_wpw = mean_squared_error(WPW, weeks * a + b, squared=False)

    plt.figure(figsize=figsize_inch)
    plt.scatter(weeks, WPW)
    plt.plot(
        weeks,
        weeks * a + b,
        c="r",
        label="$y = ax + b$ \n"
        + f"a = {a:.2f} [words-per-week]\nb = {b:.2f} [words]\nRMSE = {rmse_lineair_wpw:.2f}\n$R^2$  = {r**2:.2f}",
    )
    plt.legend(loc="upper left")
    plt.xlabel("Weeks since start of podcast")
    plt.ylabel("Released content per week [words]")
    plt.ylim(0, 33000)
    plt.title("The amount of content per week is steadily increasing")
    return rmse_lineair_wpw


rmse_lineair_wpw = plot_content_linear_fit(data)
```

That's great! We are getting more and more content every week. But, I'm not quite content with the linear trend. It is a decent fit, but if we take a closer look at the data, it doesn't actually look like the rising trend is continuing after week 90, and it is not reasonable to expect the trend to keep rising given that the show appears to have settled into the new release schedule. Therefore, a logistic regression might be a better fit for this data:

```{python}
#| label: fig-content_logistic
#| fig-cap: "A Logistic-function fitted to the Words-per-week data. The lower RMSE and higher $R^2$ indicate that this is a better fit than the linear-regression. It indicates that the initial release rate was around 8500 WPW, and it has increased to almost 22100 WPW."


def plot_content_logistic_fit(data):
    """
    Plot and show a figure of a logistic fit to the amount of Content over Time grouped by Week.

    Parameters
    ----------
    data : pd.DataFrame
        Information about the episodes.

    Returns
    -------
    _: float
        The root mean square error of the logistic fit.
    """
    START = data.iloc[0].date

    def weeks_since_start(date):
        """
        Calculate the number of weeks since START.

        START is defined outside the functions as the date of the first episode in the dataframe, typically episode 1.

        Parameters
        ----------
        date: pandas.timedelta
            The date for which to calculate the timedifference.

        Returns
        -------
        weeks_diff: int
            The number of weeks from START until date

        """
        time_diff = date - START
        weeks_diff = int(time_diff / np.timedelta64(1, "W"))
        return weeks_diff
        return weeks_diff

    def logifunc(x, A, x0, k, off):
        """
        Returns the Y value of the specified logistic function for the given X.

        Parameters
        ----------
        x : np.array
            The X values for which to compute the logistic function

        A : float
            The Amplitude of the logistic function
        x0 : float
            The x-axis shift of the logistic curve, defines the midpoint of the step in the logistic curve
        k : float
            Defines how fast the logistic curve changes
        off : float
            The y-axis offset of the logistic curve.

        Returns
        -------
        _: float
            The y value of the logistic curve at x
        """
        return A / (1 + np.exp(-k * (x - x0))) + off

    def r_squared(y, y_pred):
        """
        Calculates R-squared metric for the given input.

        Parameters
        ----------
        y : np.array
            Observed y-values
        y_pred : np.array
            Predicted y-values

        Returns
        -------
        _: float
            R-squared metric
        """
        y, y_pred = np.array(y), np.array(y_pred)
        return 1 - np.sum((y - y_pred) ** 2) / np.sum((y - np.mean(y)) ** 2)

    # Convert date to weeks since start
    data_plot = data.copy()
    data_plot = data_plot.sort_values("date")
    data_plot["weeks"] = data_plot.date.transform(lambda dt: weeks_since_start(dt))
    weeks = data_plot.weeks.unique()

    # Calculate weekly increase
    WPW = data_plot.groupby("weeks").WPE.sum()

    # Fit logistic
    popt, pcov = curve_fit(logifunc, weeks, WPW, p0=[25000, 60, 0.1, 5000])
    rmse_logistic_wpw = mean_squared_error(WPW, logifunc(weeks, *popt), squared=False)
    r_logistic_wpw = r_squared(WPW, logifunc(weeks, *popt))

    # Perform linear regression with intercept = 0
    plt.figure(figsize=figsize_inch)
    plt.scatter(weeks, WPW)
    plt.plot(
        weeks,
        logifunc(weeks, *popt),
        "r-",
        label=f"Fitted Parameters {[round(p,1) for p in popt]}\nStarting-level={popt[3]:.1f}\nEnding-level ={popt[3]+popt[0]:.1f} \nRMSE={rmse_logistic_wpw:.2f}\n $R^2$ = {r_logistic_wpw:.2f}",
    )
    plt.legend(loc="upper left")
    plt.xlabel("Weeks since start of podcast")
    plt.ylabel("Released content per week [words]")
    plt.title(
        "The amount of content per week has almost tripled since the start of the podcast!"
    )
    plt.ylim(0, 33000)
    plt.show()
    return mean_squared_error(WPW, logifunc(weeks, *popt), squared=False)


rmse_logistic_wpw = plot_content_logistic_fit(data)
```

Yes! This looks more reasonable. The content amount of content we are getting per week from Maarten and Tom has almost tripled since the podcast started. 

```{python}
#| output: asis
#| echo: false
print(f"So this podcast already has a lot of content, and we are getting a lot more every week. The total amount of content released is so far is {data.WPE.sum():.0f} words. To listen to all this content takes {data['duration-m'].sum()/60:.0f} hours.")
```
It's not only a lot of fun that we have such an impressive amount of content, it is supposedly also very helpfull for a lot of people. A funny anecdote we hear a lot about this podcast is that it is great for people who suffer from insomnia, as it helps them fall asleep. There might be a few factors at play as to why this is:

1. Maarten has a very low and calming voice.
3. There are hardly any distracting sounds or loud noises.
3. Maarten and Tom have calm conversations.
4. Maarten and talk with a consistent energy.

I feel like everybody would agree on these 4 points, but with the data I collected, we can attempt to verify points 3 and 4 with our data-driven approach.

| Content-type | Words-per-Minute                                                                     |
|--------------|:------------------------------------------------------------------------------------:|
|Presentation  | Between 100 – 150 wpm for a comfortable pace                                      |
|Conversational| Between 120 – 150 wpm                                                                |
|Audiobooks	   | Between 150 – 160 wpm (upper range that people comfortably hear & vocalize words)    |
|Radio-shows & Podcasts  | Between 150 – 160 wpm                                                 |
|Auctioneers   | Can speak at about 250 wpm                                                           |
|Commentators  | Between 250- 400 wpm                                                                 |

: Typical speech rate per content type (source: [improvepodcast](https://improvepodcast.com/words-per-minute/)) {#tbl-talkingspeed}

@tbl-talkingspeed shows us the typical words-per-minute for several types of audio-content. I couldn't find a better source than the one provided (This same table is used on multiple websites without any citations.). However, I could verify the Audibooks/Podcast rate: according to [Guidelines for the Use of Multimedia in Instruction](https://journals.sagepub.com/doi/abs/10.1177/154193129804202019) audiobooks are recommended to be at 150–160 WPM, as this is the upper-range within people comfortably hear and vocalize words, just as the table says.

So in order to very hypothesis 3 and 4, let's continue our analysis!

# 'Uitslapen' - Stay well rested like Maarten by listening to this Podcast
## Do Maarten and Tom talk fast for a podcast?
Luckily for us, calculating the average speech rate is rather trivial. We can just devide the number of words in the episode by the duration. This gives us one value per episode, and doing this for all episodes once again gives us a nice statistical distribution:

```{python}
#| label: fig-wpm_norm
#| fig-cap: "The speech rate is nicely approximated by a normal distribution. The average speech rate per episode is 164 Words-per-Minute, just above the 150-160 WPM range recommended for podcasting. 95% of the episodes lie between the range 144-184 WPM."

data["WPM"] = data["WPE"] / data["duration-m"]


def plot_wpm_histogram(data):
    """
    Plot histogram of Words-per-Minute distribution

    Parameters
    ----------
    data : pd.DataFrame
        Information about the episodes.

    Returns
    -------
    params: list
        Normal distribution fit parameters.
    """
    # Fit normal distribution
    params = norm.fit(data["WPM"])
    x = np.linspace(data["WPM"].min() - 10, data["WPM"].max() + 10, 100)
    norm_fitted = norm.pdf(x, *params)

    # Plot figure
    fig, ax = plt.subplots(figsize=figsize_inch)
    plt.title("Distribution of Words-per-Minute")
    ax.hist(data["WPM"], bins=15, density=True, edgecolor="#EAEAF2", linewidth=1)
    ax.plot(
        x,
        norm_fitted,
        label="Normal Distribution:\n"
        + r"  - $\mu$ = "
        + f"{round(params[0],2)}\n  - $\sigma$ = {round(params[1],2)}",
    )
    ax.set_xlabel("Words per minute (words/minute)")
    ax.set_ylabel("Probability Density")
    plt.xticks(rotation=70)
    plt.legend()
    plt.show()
    return params


params = plot_wpm_histogram(data)
```
Thats interesting! As shown in @fig-wpm_norm, on average Maarten and Tom talk slightly faster than the talking rate recommended for full listener comprehension.

This result suprised me. I expected the speech rate to fall nicely within this range. I couldn't find a source for dutch speaking, maybe the upper-range is dependent on the language? I did some more searching and came across this interesting publication [A comparative analysis of speech rate and perception in radio bulletins](https://emmarodero.com/researcher/projects/speech-rate/#publications) which states that the averge conversational speech rate for English is between 150-190 WPM, and the average speech rate for BBC radio is 167 WPM. Meanwhile, the speech rate for Radio Nacional de España (RNE) was 210 WPM. So the podcast speech rate is slightly slower than the BBC, which I would also typically characterize as a calm radio station. Also, it would be good to note that audiobooks also tend to be a lot calmer than regular conversations, so having a speech rate slightly faster than recommended for audibooks is probably still very calm.

In order to have some sense of a Dutch reference, I also processed the most popular Dutch podcast episode today as listed by Spotify: _"Geuze & Gorgels - season 2 episode 47 - Pretty Hurts"_. This episode has a speech rate of 207 WPM. As this single episode has a higher rate than all processed episodes for the _'Maarten van Rossem - De Podcast'_ podcast, I does give me confidence that Maarten and Tom really have a calm speech rate.

Let's move on to our next hypothesis: 'Maarten and talk in a consistent ritme.', to see if we can elucidate this subject some more.

## Maarten and Tom keep the talking rate very consistent throughout the episode
analysing this is a bit more difficult to answer, but it is certainly possible. Luckily for us `Whisper` also creates subtitle files for each processed episode (.srt, .vtt). We can process these to devide the text of each episode into minutes, so we can estimate the speech rate per minute. 
By doing this, we can estimate the speech rate distribution per episode, and plot this result as ranges per episode:

```{python}
#| label: fig-wpm_time_range
#| fig-cap: "Using the .srt (subtitle) files generated by Whisper, we are able to analyse the spread of the WPM per minute. For symmetric distributions the Highest Probability Density is the same as the mean, but for skewed distributions such as the lognormal, this value tends to be smaller than the mean and is a bit more representative for the most likely value. We can see that the episodes remain between 140-200 WPM for the most part."


def parse_srt(row):
    """
    Parse an .srt file into a dataframe with timestamps per line.

    Parameters
    ----------
    row : pd.Series
        entrie in data dataframe with episode information

    Returns
    -------
    srt: pd.DataFrame
        DataFrame with parsed .srt data
    """
    path = row.txt_path.replace("txt", "srt")
    srt = read_txt_file(path).split("\n")

    # The srt has a standard formatting:
    # 1. Line number
    # 2. Content
    # 3. Timestamp
    # Followed by a blank line
    lines, timestamps, sentences = [], [], []
    i = 0
    while i < len(srt) - 1:
        lines.append(srt[i])
        timestamps.append(srt[i + 1])
        sentences.append(srt[i + 2])
        i += 4

    # Get length of sentence
    srt = pd.DataFrame(
        {"lines": lines, "sentences": sentences, "timestamps": timestamps}
    )
    temp = srt["timestamps"].str.split("-->", n=1, expand=True)
    srt["time_start"] = temp[0]
    srt["time_start"] = srt["time_start"].str.replace(",", ".")
    srt["time_end"] = temp[1]
    srt["time_end"] = srt["time_end"].str.replace(",", ".")
    srt["time_start"] = pd.to_timedelta(srt.time_start)
    srt = srt.drop(columns="timestamps")
    srt["time_end"] = pd.to_timedelta(srt.time_end)
    srt["duration"] = pd.to_timedelta(
        srt["time_end"] - srt["time_start"]
    ).dt.total_seconds()

    # Get cadence of episode
    srt["length"] = srt["sentences"].transform(lambda text: get_length(text))

    # Aggregate per minute
    srt["minute"] = (srt.time_end.dt.total_seconds() // 60).astype("int")
    srt = srt.groupby("minute").agg(
        {"sentences": " ".join, "duration": "sum", "length": "sum"}
    )
    srt["WPM_line"] = srt["length"] / srt["duration"] * 60
    srt = srt.reset_index()
    srt["episode"] = path
    srt["date"] = row.date
    srt["titles"] = row.titles

    # We drop the last minute as it contains the outro and adds.
    # We also drop the first minute, as it contains the intro
    srt = srt[1:-1]
    # In a few files, Whisper had trouble picking up the audio.
    # Only transcribing 1 line for more than a minute.
    # To filter these bad lines out, we remove any minute with less than 90 words.and
    # Also, in some episodes they listen to historic speeches, which are quite slow.
    # We filter these 'bad'-lines out by setting a lower limit of 100 WPM
    # This is not a problem, as it is far below even the lower bound of the speech rate.
    srt = srt[srt.WPM_line >= 100]
    return srt


def plot_srts_wpm(srts):
    """
    Plot wpm distribution over time, using kernel density estimators.

    Parameters
    ----------
    srts : list(pd.DataFrames)
        A parsed .srt DataFrame per episode

    Returns
    -------
    ci_90: np.array
        90% confidence interval per episode (not HPDI).
    hpd: np.array
        WPM with highest probability density per episode.
    """
    ci_60 = []
    ci_90 = []
    hpd = []
    date = []
    for df in srts:
        # Calculate the data confidence interval
        kde = KDEUnivariate(df.WPM_line)
        kde.fit(bw=5)
        icdf = kde.icdf
        confidence_interval = (np.percentile(icdf, 20), np.percentile(icdf, 80))
        ci_60.append(confidence_interval)
        confidence_interval = (np.percentile(icdf, 5), np.percentile(icdf, 95))
        ci_90.append(confidence_interval)
        hpd.append(kde.support[np.argmax(kde.density)])
        date.append(df.date[1])
    ci_60 = np.array(ci_60)
    ci_90 = np.array(ci_90)

    fig, ax = plt.subplots(figsize=figsize_inch)
    plt.plot(date, hpd, alpha=0.5, color="k", label="Highest Probability Density WPM")
    plt.fill_between(
        date,
        ci_90[:, 0],
        ci_90[:, 1],
        color="#1f77b4",
        linewidth=0,
        alpha=0.3,
        label=r"90% interval WPM",
    )
    plt.fill_between(
        date,
        ci_60[:, 0],
        ci_60[:, 1],
        color="#1f77b4",
        linewidth=0,
        alpha=0.5,
        label=r"60% interval WPM",
    )

    plt.xlabel("Episode #")
    plt.ylabel("Words-per-Minute")
    plt.title("Confidence Interval of WPM_lines")
    plt.legend()
    plt.xticks(rotation=70)
    ax.xaxis.set_major_locator(mdates.MonthLocator())
    ax.xaxis.set_major_formatter(mdates.DateFormatter("%Y-%b"))
    plt.show()
    return ci_90, hpd


srts = []
for _, row in data.iterrows():
    srt = parse_srt(row)
    srts.append(srt)
ci_90, hpd = plot_srts_wpm(srts)
```
It is fun to see that @fig-wpe_time and @fig-wpm_time_range show a similair change in episode structure. The earlier episodes had a speech rate almost 20 wpm faster than more recent episodes. Other than that, we can see that episodes stay within the range of 150-180 WPM, with a very small percentage going above 200 wpm. #TODO

```{python}
#| label: fig-wpm_time_range_weekdays
#|fig-cap: "Grouping these episodes by the respective day of the week, we can see a pattern emerge. Tuesday episodes tend to be the slowest, while Monday and Saturday episodes tend to be the fastest. Wednesday episodes have the smallest spread thus appear to be the most consistent, while also being fairly slow. Thus Tuesday and Wednesday episodes are perhaps the best choice for those trying to sleep."


def plot_srts_wpm_weekly(srts):
    """
    Plot wpm distribution over per weekday, ing kernel density estimators.

    Parameters
    ----------
    srts : list(pd.DataFrames)
        A parsed .srt DataFrame per episode

    Returns
    -------
    ci_90: np.array
        90% confidence interval per weekday (not HPDI).
    hpd: np.array
        WPM with highest probability density per weekday.
    """

    # plot per day of week
    df_weekday = pd.concat(srts)
    df_weekday["weekday"] = df_weekday.date.transform(lambda x: x.weekday())

    ci_60 = []
    ci_90 = []
    hpd = []
    weekdays = []
    number_to_week = {
        1: "Monday",
        2: "Tuesday",
        3: "Wednesday",
        4: "Thursday",
        5: "Friday",
        6: "Saturday",
        7: "Sunday",
    }
    for i in range(7):
        weekday = number_to_week[i + 1]
        df = df_weekday[df_weekday.weekday == i]
        # Calculate the data confidence interval
        kde = KDEUnivariate(df.WPM_line)
        kde.fit(bw=6)
        icdf = kde.icdf
        confidence_interval = (np.percentile(icdf, 20), np.percentile(icdf, 80))
        ci_60.append(confidence_interval)
        confidence_interval = (np.percentile(icdf, 5), np.percentile(icdf, 95))
        ci_90.append(confidence_interval)
        hpd.append(kde.support[np.argmax(kde.density)])
        weekdays.append(weekday)

    ci_60 = np.array(ci_60)
    ci_90 = np.array(ci_90)

    # Set horizontal labels
    labels = df_weekday.groupby("weekday").agg(
        {"weekday": lambda x: x.iloc[0], "episode": pd.Series.nunique}
    )
    labels["label"] = labels.agg(
        lambda row: f"{number_to_week[row.weekday+1]}\n{row.episode} episodes", axis=1
    )

    fig, ax = plt.subplots(figsize=figsize_inch)
    plt.plot(
        weekdays, hpd, alpha=0.5, color="k", label="Highest Probability Density WPM"
    )
    plt.fill_between(
        weekdays,
        ci_90[:, 0],
        ci_90[:, 1],
        color="#1f77b4",
        linewidth=0,
        alpha=0.3,
        label=r"90% interval WPM",
    )
    plt.fill_between(
        weekdays,
        ci_60[:, 0],
        ci_60[:, 1],
        color="#1f77b4",
        linewidth=0,
        alpha=0.5,
        label=r"60% interval WPM",
    )
    plt.xlabel("Weekday")
    plt.ylabel("Words-per-Minute")
    plt.title("Confidence Interval of WPM_lines")
    plt.legend()
    plt.gca().set_xticklabels(labels.label)
    # plt.xticks(rotation = 70)
    # ax.xaxis.set_major_locator(mdates.MonthLocator())
    # ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%b'))
    plt.show()
    return ci_90, hpd


# Again, we only select the second half of the show
srts_second_half = [
    srt for second, srt in zip((data["date"] > "2021-08-1"), srts) if second
]
ci_90, hpd = plot_srts_wpm_weekly(srts_second_half)
```

In @fig-wpm_time_range_weekdays a cool weekly pattern emerges. On Tuesday and Wednesday episodes are the slowest, while saturday episodes are clearly the fastest. Also, in general the spread shown in @fig-wpm_time_range_weekdays in @fig-wpm_time_range appears to be fairly narrow. 60% of the episodes stays within a 30 WPM band, and hardly ever do they go above 200 WPM.

While part of me would like a more robust comparisson with some other data, I think we've gone far enough on this topic. If you have trouble sleeping, the data seems to agree that listening to this podcast might be an excellent choice. If you are interested in a curated playlist of the 'slowest' episodes, click on the cell below!

::: {.callout-note collapse="true"}
## The top 25 'slowest' episodes for helping you sleep
```{python}
#| echo: false
playlist = data.sort_values("WPM").head(25)[print_cols+["WPM"]]
playlist.to_html(index=False)
```
:::

It's time to delve deeper into our data. Let's make use of all the text-data we've collected!

# 'Nou natuurlijk beste mensen!' - are of course some of the favourite words of Maarten {#sec-wordclouds}
Maarten has a typical way of speaking, you don't need any data to know this. Especially when he gets excited. As we have almost 200 hours of Maarten Talking, we can figure out what his most commonly used words are!

```{python}
#| label: fig-wordcloud_full
#| fig-cap: "A wordcloud of the most common words in the podcast. The size of a word indicates the relative frequency. Since we didn't filter for stopwords, the wordcloud is filled with uninteresting words that are not specific for Maarten."
full_text = " ".join(data["text"]).lower()
path_wordcloud_full = "./cache/wordcloud_full.pickle"

if not os.path.exists(path_wordcloud_full):
    # Generate a word cloud image
    wordcloud_full = WordCloud(
        width=figsize[0],
        height=figsize[1],
        include_numbers=True,
        random_state=random_seed,
        background_color="white",
        relative_scaling=1,
        max_words=300,
    ).generate(full_text)
    write_pickle(path_wordcloud_full, wordcloud_full)
else:
    wordcloud_full = read_pickle(path_wordcloud_full)

# Display the generated image:
# the matplotlib way:
plt.figure(figsize=figsize_inch)
plt.imshow(wordcloud_full, interpolation="bilinear")
plt.axis("off")
plt.tight_layout()
plt.show()
```
Right.. this is not what we wanted. It is logical that these words appear a lot, but they don't have any interesting meaning. We need to filter out these words, called stopwords, to get a more interesting result.

```{python}
#| label: fig-wordcloud_stopwords
#| fig-cap: "A wordcloud of the most common words in the podcast. The size of a word indicates the relative frequency. We can see common words appear, which also indicate various topics Maarten and Tom talk about: Ukraine, politics, Trump, history. Take a look and see what you can spot and recognize!"

# Create stopword list:
stopwords_path = "../dependencies/dutch_stopwords.txt"
with open(stopwords_path, "r") as f:
    dutch_stopwords = f.read().split("\n")
dutch_stopwords = set(dutch_stopwords)

# Create improved wordcloud
path_wordcloud_stopwords = "./cache/wordcloud_stopwords.pickle"
if not os.path.exists(path_wordcloud_stopwords):
    wordcloud_stopwords = WordCloud(
        stopwords=dutch_stopwords,
        width=figsize[0],
        height=figsize[1],
        include_numbers=True,
        random_state=random_seed,
        background_color="white",
        relative_scaling=1,
        max_words=300,
    ).generate(full_text)
    write_pickle(path_wordcloud_stopwords, wordcloud_stopwords)
else:
    wordcloud_stopwords = read_pickle(path_wordcloud_stopwords)
# Display the generated image:
# the matplotlib way:
plt.figure(figsize=figsize_inch)
plt.imshow(wordcloud_stopwords, interpolation="bilinear")
plt.axis("off")
plt.tight_layout()
plt.show()
```

That's more like it! It appears that Maarten uses the word 'Natuurlijk' a lot. Also, I can recognize the use of 'Mensen', 'Allemaal', and 'Helemaal' from some of the typical Maarten rants when he gets annoyed by something. There are also all kinds of topics hidden in the wordcloud. Take a look for yourself!

## Table of most common words
While figures are always fine, it's also nice to look at the raw-data itself.
```{python}
def get_top_n_words(data):
    """
    List all words in the corpus that are used on average >= 1 time per episode.

    Parameters
    ----------
    data : pd.DataFrame
        Information about the episodes.

    Returns
    -------
    word_freq : pd.DataFrame
        Table of word frequencies.
    bag_of_words : pd.DataFrame
        Table containing the word frequencies per episode.
    vec : sklearn.CountVectorizer
        CountVectorizer fitted to the corpus.
    """
    vec = CountVectorizer(ngram_range=(1, 5), stop_words=dutch_stopwords).fit(
        data["text"]
    )
    bag_of_words = vec.transform(data["text"])
    sum_words = bag_of_words.sum(axis=0)
    word_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]
    word_freq = sorted(word_freq, key=lambda x: x[1], reverse=True)
    word_freq = pd.DataFrame(word_freq, columns=("Token", "Count"))
    word_freq["Rate_per_Episode"] = round(word_freq["Count"] / data.shape[0], 2)
    word_freq = word_freq[word_freq["Rate_per_Episode"] >= 1]
    return word_freq, bag_of_words, vec


path_top_n_words = "./cache/top_n_words.pickle"
if not os.path.exists(path_top_n_words):
    word_freq, bag_of_words, vectorizer = get_top_n_words(data)
    package = [word_freq, bag_of_words, vectorizer]
    write_pickle(path_top_n_words, package)
else:
    package = read_pickle(path_top_n_words)
    word_freq, bag_of_words, vectorizer = package
pd.set_option("display.max_rows", word_freq.shape[0] + 1)
word_freq.iloc[:21].to_html(index=False)
```

We knew 'Natuurlijk' was used a lot, but 32 times per episode, wow!
I've collected all words used more than once per episode in the table below.

::: {.callout-note collapse="true"}
## Click here for the full table with Rate_per_Episode >= 1
```{python}
#| echo: false
word_freq.to_html(index=False)
```
:::

As we noted above, some interesting topics arouse. I wonder, would it be possible to track some interesting topics over time? For example, I can't image Maarten and Tom talked a lot about Ukraine before the war, so we should be able to pick this up in the data.

Let's attempt to track the use of "oekraïne" across episodes:
```{python}
#| label: fig-over_time_oekraine
#| fig-cap: "In this figure we can clearly see the interest in the topic of the Ukraine take off in januari 2022. This demonstrates that we can track topics over time in the podcast."
def plot_word_over_time(target_word):
    """
    Plot the frequency of the target word over time.

    Parameters
    ----------
    target_word : str
        Word of interest.
    """
    id = [idx for word, idx in vectorizer.vocabulary_.items() if word == target_word][0]
    dist = bag_of_words[:, id].toarray().T[0]

    fig, ax = plt.subplots(figsize=figsize_inch)
    plt.plot(data["date"], dist)
    plt.title(f"Mentions of the word: {target_word}")
    plt.ylabel("Counts")
    plt.xlabel("Date")

    # set x-axis
    ax.xaxis.set_major_locator(mdates.MonthLocator())
    ax.xaxis.set_major_formatter(mdates.DateFormatter("%Y-%b"))
    plt.xticks(rotation=70)
    plt.show()


target_word = "oekraïne"
plot_word_over_time(target_word)
```

@fig-over_time_oekraine demonstrates that we can indeed track topics over time.
Let us try this for another topic:
```{python}
#| label: fig-over_time_ggd
#| fig-cap: "While less extreme than @fig-over_time_oekraine, we can clearly see our key event in the data."
plot_word_over_time("ggd")
```
As some of you may Remember, Maarten had a small run-in with the GGD and Hugo de Jonge were it was later revealed that Maarten was fully justified in his actions. As far as I know he never received an apalogy from Hugo... Anyways, this hole stint happened roughly halfway March 2021, and in @fig-over_time_ggd we see a big spike in the mentions of the GGD.

So, we've demonstrated we can track news-worthy topics over time by mining our podcast. Can we go deeper? Yes, quite a bit in fact! In the next section we'll use some clever technique's to find out what this podcast is actually about.

# 'Kunt u mij horen?' - What is 'Maarten van Rossem - De Podcast' actually about? {#sec-topic}
Within the field of machine learning, there is the field of 'topic modeling'. Topic modeling tries to automatically extract abstract the abstract concept of topics from a collection of documents, and then asigns one or more topics to each document. So given a selection of documents, such as transcribed podcast episodes, we can use these unsupervised machine learning techniques to extract common topics from the various episodes. 

This part of the analysis was largely inspired by the wonderful work of [Maarten Grootendorst](https://github.com/MaartenGr/BERTopic) and his `BERTopic` topic modeling pipeline.

Getting a topic per episode is nice, but if there is one thing I know about this podcast, it is that it can wildly swing off topic at any moment. So instead of getting a topic per sentence, it would be more interesting to get a topic per episode. And since we already wrote some code to chop the .srt files into minutes, this is actualy quite easy to do! The relatively short chunk of code below performs all the steps for extracting our topics.

```{python}
# Sentencize the transcripts and track their titles
docs = []
titles = []
timestamps = []
for srt in srts:
    for text_minute in srt["sentences"]:
        docs.append(text_minute)
        titles.append(srt.titles.iloc[1])
        timestamps.append(srt.date.iloc[1])

# Create sentence embeddings for BERTopic
embeddings_path = "./cache/embeddings.npy"
if not os.path.exists(embeddings_path):
    # Create embeddings from the documents
    sentence_model = SentenceTransformer("paraphrase-multilingual-mpnet-base-v2")
    embeddings = sentence_model.encode(docs)
    np.save(embeddings_path, embeddings)
else:
    embeddings = np.load(embeddings_path)

# Train and save our BERTopic model, load from cache if save file is found
model_path = "./cache/BERTopic_topic_model.pickle"
if not os.path.exists(model_path):
    # Reset random seeds
    random.seed(random_seed)
    np.random.seed(random_seed)

    # Define sub-models
    sentence_model = SentenceTransformer("paraphrase-multilingual-mpnet-base-v2")
    vectorizer = CountVectorizer(stop_words=dutch_stopwords)
    umap_model = UMAP(
        n_neighbors=15,  # Higher -> More global structure, less local
        n_components=7,  # Effects clustering mostly, advice is to leave around ~5
        min_dist=0.0,
        metric="cosine",
        random_state=random_seed + 1,
    )

    hdbscan_model = HDBSCAN(
        min_cluster_size=15,  # Higher -> less clusters, bigger size
        min_samples=4,  # Lower -> less outlier classifications, may be detrimental to quality topics.
        metric="euclidean",
        cluster_selection_method="eom",
    )

    # Train our topic model
    topic_model = BERTopic(
        n_gram_range=(1),
        embedding_model=sentence_model,
        umap_model=umap_model,
        hdbscan_model=hdbscan_model,
        vectorizer_model=vectorizer,
    )

    topic_model = topic_model.fit(docs, embeddings)

    # Save model
    topic_model.save(model_path)
else:
    topic_model = BERTopic().load(model_path)
```

## What topics does the model find?
Our model has tried to assigned each minute of the podcast, to a topic, excluding the first and last minute of each episode. The top 10 most assigned topics are shown in the table below. The name of the topic is formed by selecting the top 4 words that appear a lot in the ontopic documents and very little in the offtopic documents:

```{python}
topics = topic_model.get_topic_info()
print(len(topics))
topics[:16]
# topics[1:11].to_html(index=False)
```
Amazing! We can see that the model was able to find clearly interpretable topics, and they make a lot of sense. Maarten and Tom have talked a lot about the Ukraine and Russia, as well as Dutch politics, the polics of the USA, Covid-19, the Netherlands. I'm realy impressed with how well this works.

::: {.callout-note collapse="true"}
## Click here for all the topics identified by the model
The first topic **-1** collects all documents, or in our case minutes, that could not be assigned to any particular topic. As you will see later in the visualization, these still cluster nicely around known topics, and using this their topic can often still be infered.
```{python}
topics = topic_model.get_topic_info()
topics.to_html(index=False)
```
:::

To demonstrate this further, we can check some of the classified minutes for the first 3 topics:

```{python}
#| output: asis
for topic_i in range(1, 4):
    print(f"The topic is defined by:  {topics.iloc[topic_i]['Name']}" + "\n")
    example = " ".join(topic_model.get_representative_docs()[topic_i-1])
    print(f"Representative minute of podcast:\n\n {example}" + "\n\n")
    print("----------------------------------------------------------\n\n")
```

Yep, those fit really well! Something really cool we can do next is to visualize all these topics to also give you a sense of how they are all related.

::: {.callout-note collapse="true"}
## Click here for all the results up to topic 20
```{python}
#| echo: false
#| output: asis

for topic_i in range(1, 21):
    print(f"The topic is defined by:  {topics.iloc[topic_i]['Name']}" + "\n")
    example = " ".join(topic_model.get_representative_docs()[topic_i-1])
    print(f"Representative minute of podcast:\n\n {example}" + "\n\n")
    print("----------------------------------------------------------\n\n")
```
:::

## How does the model think these are related?
The following figure is really cool! Our model has assigned topics to our documents by performing clustering in the latent space of topics. In this latent space numbers are tied to topics, thus moving in one direction means you move towards a certain topic. Of course this is a very quic-and-dirty explanation, but this means that we can also visualize our documents in this fasion. Of course we cannot visualize 728 dimensions, the dimensionality of the latent space, so we map this into 2 dimensions. Now, this means you lose a lot of information. But, some structure is retained which means that topics that are closer together are more related, and topics that are far apart are less related. Enough talking, how about you explore this interactive figure yourself!

```{python}
#| column: page 
#| label: fig-topics_documents
#| fig-cap: "Explore the latent embedding and clustering of documents yourself in this interactive figure!"
# Generate nicer looking labels and set them in our model
topic_labels = topic_model.generate_topic_labels(
    nr_words=3, topic_prefix=False, word_length=25, separator=","
)
topic_model.set_topic_labels(topic_labels)

# Create reduced embeddings we can reuse for consistency
reduced_embeddings_path = "./cache/reduced_embeddings.npy"
if not os.path.exists(reduced_embeddings_path) or 1:
    umap_model_2d = UMAP(
        n_neighbors=10,
        n_components=2,
        min_dist=0.0,
        metric="cosine",
        random_state=random_seed,
    ).fit(embeddings)
    reduced_embeddings = umap_model_2d.embedding_
    np.save(reduced_embeddings_path, reduced_embeddings)
else:
    reduced_embeddings = np.load(reduced_embeddings_path)

# Manually selected some interesting topics to prevent information overload
topics_of_interest = list(range(0, 100))

# I added the title to the documents themselves for easier interactivity
adjusted_docs = [
    "<b>" + title + "</b><br>" + doc[:100] + "..." for doc, title in zip(docs, titles)
]

# Visualize documents
topic_model.visualize_documents(
    adjusted_docs,
    reduced_embeddings=reduced_embeddings,
    hide_annotations=False,
    topics=topics_of_interest,
    custom_labels=True,
    width=figsize[0],
    height=figsize[1],
)
```

There are some clear regions in the figure, such as:

* lower left: Various types of media
* center bottom: Politics and Elections
* center right: Wars, from historic to present day
* upper right (below Taiwan): Climate, energie, nitrogen-crisis
* center top: Technology
* center top (below Technology): Transportation

So this has also given us a pretty nice overview of the main topics of the podcast are:

* Dutch politics
* American Politics
* China
* Technology and Science
* The History of Technology and Science
* Traveling
* Media
* World War I, World War II, Cold War

Another interesting property we can investigate is how all the topics are related in a hierarchical sense. Once again we can see quite some logical patterns popping up along the way:

```{python}
#| label: fig-topics_hierarchy
#| fig-cap: "This cluster hierarchy plot visualizes how the various topics are related. The shorter you have to travel to the right before topics merge, the closer related they are according to the model."
#| column: page
hierarchical_topics = topic_model.hierarchical_topics(docs)

topic_model.visualize_hierarchy(
    hierarchical_topics=hierarchical_topics, 
    width = figsize[0],
    height = figsize[1]
)
```

If you hover over the black circles, you will see the topic representation at that level of the hierarchy. These representations help you understand the effect of merging certain topics together. Some might be logical to merge whilst others might not. Moreover, we can now more clearly see which sub-topics can be found and how they are related in the larger scheme of things.

```{python}
#| label: fig-topics_document_and_hierarchy
#| fig-cap: "This figure combines the document topic visualization and the hierarchical visualization. This figure doesn't have a hover over as this add a lot to the filesize of this document."
#| column: page 
topic_model.visualize_hierarchical_documents(
    docs, 
    hierarchical_topics, 
    reduced_embeddings=reduced_embeddings,
    width = figsize[0],
    height = figsize[1]
)
```

@fig-topics_document_and_hierarchy combines @fig-topics_documents and @fig-topics_hierarchy into a single figure. Showing us the what the hierarchical clustering looks like in the 2 dimensional representation.

## How are the top 10 topics distributed over time?

```{python}
#| column: page 
#| label: fig-topic_per_week_over_time
#| fig-cap: "The top 12 most occuring topics of the podcast as visualized over time. We can see several big events in the data, such as the presidential elections in the USA in 2020, the invasion of Ukraine in 2022, and the farmers protests in the summer of 2022."

def topics_over_time_weeks(
    topic_model,
    docs: List[str],
    timestamps: Union[List[str], List[int]],
    binsize: int = None,
    datetime_format: str = None,
    evolution_tuning: bool = True,
    global_tuning: bool = True,
):
    """
    Create topics over time, aggregated to the bi-weekly level.

    (Please note, this function is a direct addaptation of the sourcecode of BERTopic)
    To create the topics over time, BERTopic needs to be already fitted once.
    From the fitted models, the c-TF-IDF representations are calculate at
    each timestamp t. Then, the c-TF-IDF representations at timestamp t are
    averaged with the global c-TF-IDF representations in order to fine-tune the
    local representations.
    NOTE:
        Make sure to use a limited number of unique timestamps (<100) as the
        c-TF-IDF representation will be calculated at each single unique timestamp.
        Having a large number of unique timestamps can take some time to be calculated.
        Moreover, there aren't many use-cased where you would like to see the difference
        in topic representations over more than 100 different timestamps.
    Arguments:
        docs: The documents you used when calling either `fit` or `fit_transform`
        timestamps: The timestamp of each document. This can be either a list of strings or ints.
                    If it is a list of strings, then the datetime format will be automatically
                    inferred. If it is a list of ints, then the documents will be ordered by
                    ascending order.
        binsize:    The binsize you want to create for the timestamps.
                    The binsize is given in number of weeks, the default is 2.
        datetime_format: The datetime format of the timestamps if they are strings, eg “%d/%m/%Y”.
                         Set this to None if you want to have it automatically detect the format.
                         See strftime documentation for more information on choices:
                         https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior.
        evolution_tuning: Fine-tune each topic representation at timestamp *t* by averaging its
                          c-TF-IDF matrix with the c-TF-IDF matrix at timestamp *t-1*. This creates
                          evolutionary topic representations.
        global_tuning: Fine-tune each topic representation at timestamp *t* by averaging its c-TF-IDF matrix
                   with the global c-TF-IDF matrix. Turn this off if you want to prevent words in
                   topic representations that could not be found in the documents at timestamp *t*.
    Returns:
        topics_over_time: A dataframe that contains the topic, words, and frequency of topic
                          at timestamp *t*.
    """
    documents = pd.DataFrame(
        {"Document": docs, "Topic": topic_model.topics_, "Timestamps": timestamps}
    )
    global_c_tf_idf = normalize(topic_model.c_tf_idf_, axis=1, norm="l1", copy=False)

    all_topics = sorted(list(documents.Topic.unique()))
    all_topics_indices = {topic: index for index, topic in enumerate(all_topics)}

    if isinstance(timestamps[0], str):
        infer_datetime_format = True if not datetime_format else False
        documents["Timestamps"] = pd.to_datetime(
            documents["Timestamps"],
            infer_datetime_format=infer_datetime_format,
            format=datetime_format,
        )

    # extract years from timestamps
    years = np.unique(data.date.dt.strftime("%Y").astype(int))
    end_year = pd.to_datetime(f"{years[-1]+1}-01-01")
    # Get timestamps for every week of the year
    bin_edges = []
    for year in years:
        for week in range(1, 54, binsize):
            date = f"{year}-{week}-1"
            date = pd.to_datetime(date, format="%Y-%W-%w")
            bin_edges.append(date)
    bin_edges = pd.DataFrame(bin_edges, columns=["bin_edges"])
    # remove year overflow at the end
    bin_edges = bin_edges[bin_edges.bin_edges < end_year]
    # remove year overflow inbetween
    bin_edges = bin_edges.drop_duplicates()

    # bin the timestamps
    documents["Bins"] = pd.cut(documents.Timestamps, bins=bin_edges.bin_edges.values)
    documents["Timestamps"] = documents.apply(lambda row: row.Bins.left, 1)

    # Sort documents in chronological order
    documents = documents.sort_values("Timestamps")
    timestamps = documents.Timestamps.unique()
    if len(timestamps) > 200:
        warnings.warn(
            f"There are more than 200 unique timestamps (i.e., {len(timestamps)}) "
            "which significantly slows down the application. Consider setting `nr_bins` "
            "to a value lower than 100 to speed up calculation. "
        )

    # For each unique timestamp, create topic representations
    topics_over_time = []
    for index, timestamp in tqdm(
        enumerate(timestamps), disable=not topic_model.verbose
    ):

        # Calculate c-TF-IDF representation for a specific timestamp
        selection = documents.loc[documents.Timestamps == timestamp, :]
        documents_per_topic = selection.groupby(["Topic"], as_index=False).agg(
            {"Document": " ".join, "Timestamps": "count"}
        )
        c_tf_idf, words = topic_model._c_tf_idf(documents_per_topic, fit=False)

        if global_tuning or evolution_tuning:
            c_tf_idf = normalize(c_tf_idf, axis=1, norm="l1", copy=False)

        # Fine-tune the c-TF-IDF matrix at timestamp t by averaging it with the c-TF-IDF
        # matrix at timestamp t-1
        if evolution_tuning and index != 0:
            current_topics = sorted(list(documents_per_topic.Topic.values))
            overlapping_topics = sorted(
                list(set(previous_topics).intersection(set(current_topics)))
            )

            current_overlap_idx = [
                current_topics.index(topic) for topic in overlapping_topics
            ]
            previous_overlap_idx = [
                previous_topics.index(topic) for topic in overlapping_topics
            ]

            c_tf_idf.tolil()[current_overlap_idx] = (
                (
                    c_tf_idf[current_overlap_idx]
                    + previous_c_tf_idf[previous_overlap_idx]
                )
                / 2.0
            ).tolil()

        # Fine-tune the timestamp c-TF-IDF representation based on the global c-TF-IDF representation
        # by simply taking the average of the two
        if global_tuning:
            selected_topics = [
                all_topics_indices[topic] for topic in documents_per_topic.Topic.values
            ]
            c_tf_idf = (global_c_tf_idf[selected_topics] + c_tf_idf) / 2.0

        # Extract the words per topic
        labels = sorted(list(documents_per_topic.Topic.unique()))
        words_per_topic = topic_model._extract_words_per_topic(words, c_tf_idf, labels)
        topic_frequency = pd.Series(
            documents_per_topic.Timestamps.values, index=documents_per_topic.Topic
        ).to_dict()

        # Fill dataframe with results
        topics_at_timestamp = [
            (
                topic,
                ", ".join([words[0] for words in values][:5]),
                topic_frequency[topic],
                timestamp,
            )
            for topic, values in words_per_topic.items()
        ]
        topics_over_time.extend(topics_at_timestamp)

        if evolution_tuning:
            previous_topics = sorted(list(documents_per_topic.Topic.values))
            previous_c_tf_idf = c_tf_idf.copy()

    return pd.DataFrame(
        topics_over_time, columns=["Topic", "Words", "Frequency", "Timestamp"]
    )


def stacked_area_chart_topics(
    topic_model,
    topics_over_time: pd.DataFrame,
    top_n_topics: int = 10,
    topics: List[int] = None,
    normalize_frequency: bool = False,
    custom_labels: bool = False,
    width: int = figsize[0],
    height: int = figsize[1],
):
    """
    Visualize topics over time, normalized per timestep

    (Please note, this function is a direct addaptation of the sourcecode of BERTopic)
    Arguments:
        topic_model: A fitted BERTopic instance.
        topics_over_time: The topics you would like to be visualized with the
                          corresponding topic representation
        top_n_topics: To visualize the most frequent topics instead of all
        topics: Select which topics you would like to be visualized
        custom_labels: Whether to use custom topic labels that were defined using
                       `topic_model.set_topic_labels`.
        width: The width of the figure.
        height: The height of the figure.
    Returns:
        A plotly.graph_objects.Figure including all traces
    """
    colors = plt.cm.tab20(np.linspace(0, 1, 20))
    colors = [rgb2hex(color) for color in colors]
    # Select topics based on top_n and topics args
    freq_df = topic_model.get_topic_freq()
    freq_df = freq_df.loc[freq_df.Topic != -1, :]
    if topics is not None:
        selected_topics = list(topics)
    elif top_n_topics is not None:
        selected_topics = sorted(freq_df.Topic.to_list()[:top_n_topics])
    else:
        selected_topics = sorted(freq_df.Topic.to_list())

    # Prepare data
    if topic_model.custom_labels_ is not None and custom_labels:
        topic_names = {
            key: topic_model.custom_labels_[key + topic_model._outliers]
            for key, _ in topic_model.topic_labels_.items()
        }
    else:
        topic_names = {
            key: value[:40] + "..." if len(value) > 40 else value
            for key, value in topic_model.topic_labels_.items()
        }
    topics_over_time["Name"] = topics_over_time.Topic.map(topic_names)
    data = topics_over_time.loc[
        topics_over_time.Topic.isin(selected_topics), :
    ].sort_values(["Topic", "Timestamp"], ascending=False)

    # Add traces
    fig = go.Figure()
    for index, topic in enumerate(data.Topic.unique()):
        trace_data = data.loc[data.Topic == topic, :]
        topic_name = trace_data.Name.values[0]
        words = trace_data.Words.values
        y = trace_data.Frequency
        if index == 0:
            fig.add_trace(
                go.Scatter(
                    x=trace_data.Timestamp,
                    y=y,
                    mode="lines",
                    line={"color": colors[index]},
                    hoverinfo="text",
                    name=topic_name,
                    stackgroup="one",
                    groupnorm="percent",
                    hovertext=[
                        f"<b>Topic {topic}</b><br>Words: {word}" for word in words
                    ],
                )
            )
        else:
            fig.add_trace(
                go.Scatter(
                    x=trace_data.Timestamp,
                    y=y,
                    mode="lines",
                    line={"color": colors[index]},
                    hoverinfo="text",
                    name=topic_name,
                    stackgroup="one",
                    hovertext=[
                        f"<b>Topic {topic}</b><br>Words: {word}" for word in words
                    ],
                )
            )
    # Styling of the visualization
    fig.update_xaxes(showgrid=True, dtick="M1", tickformat="%Y-%b")
    fig.update_yaxes(showgrid=True, ticksuffix="%")
    fig.update_layout(
        yaxis_title="Normalized Frequency",
        yaxis_range=(0, 100),
        title={
            "text": f"<b>Topics over Time - Interval size of {binsize} weeks",
            "y": 0.93,
            "x": 0.40,
            "xanchor": "center",
            "yanchor": "top",
            "font": dict(size=25, color="Black"),
        },
        template="simple_white",
        width=width,
        height=height,
        hoverlabel=dict(bgcolor="white", font_size=16, font_family="Rockwell"),
        legend=dict(
            title="<b>Global Topic Representation",
        ),
    )
    return fig


binsize = 2
topics_over_time = topics_over_time_weeks(
    topic_model, docs, timestamps, binsize=binsize
)
stacked_area_chart_topics(topic_model, topics_over_time, top_n_topics=12)
```

We end it here with @fig-topic_per_week_over_time #Todo finish